name: Database Export and Commit
on:
  workflow_dispatch:
    inputs:
      export_mode:
        description: "Export mode"
        required: true
        default: "all-tables"
        type: choice
        options:
          - all-tables
          - repo-only
          - stats-only
      commit_message:
        description: "Custom commit message (optional)"
        required: false
        default: ""
        type: string

jobs:
  export-and-commit:
    runs-on: ubuntu-latest

    services:
      postgres:
        image: postgres:14
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: github_crawler
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5

    steps:
      - name: Checkout Repository
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.GITHUB_TOKEN }}
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install Dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          pip install psycopg2-binary

      - name: Setup Database Schema
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/github_crawler
        run: |
          echo "Setting up database schema..."
          psql $DATABASE_URL -f migrations/001_initial_schema.sql
          psql $DATABASE_URL -f migrations/002_add_alphabet_partition.sql
          echo "âœ… Database schema setup complete"

      - name: Download All Matrix Crawler Artifacts
        uses: actions/download-artifact@v3
        with:
          path: ./artifacts
        continue-on-error: true

      - name: Import Artifacts to Database
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/github_crawler
        run: |
          echo "ğŸ”„ Importing artifacts to database..."

          # Create import script
          cat > import_artifacts.py << 'EOF'
          import os
          import csv
          import psycopg2
          from datetime import datetime
          import glob

          DATABASE_URL = os.getenv('DATABASE_URL')
          conn = psycopg2.connect(DATABASE_URL)
          cur = conn.cursor()

          # Find all CSV files in artifacts
          csv_files = glob.glob('./artifacts/**/final_*_stars_data.csv', recursive=True)
          print(f"Found {len(csv_files)} CSV files to import")

          imported_count = 0
          for csv_file in csv_files:
              print(f"Importing {csv_file}...")
              try:
                  with open(csv_file, 'r') as f:
                      reader = csv.DictReader(f)
                      for row in reader:
                          # Insert repo data
                          cur.execute("""
                              INSERT INTO repo (id, name, owner, url, created_at, alphabet_partition)
                              VALUES (%s, %s, %s, %s, %s, %s)
                              ON CONFLICT (id) DO UPDATE SET
                                  name = EXCLUDED.name,
                                  owner = EXCLUDED.owner,
                                  url = EXCLUDED.url,
                                  alphabet_partition = EXCLUDED.alphabet_partition
                          """, (
                              int(row.get('id', 0)) or hash(f"{row['owner']}/{row['name']}") % 2147483647,
                              row['name'],
                              row['owner'],
                              f"https://github.com/{row['owner']}/{row['name']}",
                              datetime.now(),
                              row.get('alphabet_partition', '')
                          ))
                          
                          # Insert repo stats
                          cur.execute("""
                              INSERT INTO repo_stats (repo_id, fetched_date, stars)
                              VALUES (%s, %s, %s)
                              ON CONFLICT (repo_id, fetched_date) DO UPDATE SET
                                  stars = EXCLUDED.stars
                          """, (
                              int(row.get('id', 0)) or hash(f"{row['owner']}/{row['name']}") % 2147483647,
                              row.get('fetched_date', datetime.now().date()),
                              int(row['stars'])
                          ))
                          
                          imported_count += 1
                          
              except Exception as e:
                  print(f"Error importing {csv_file}: {e}")
                  continue

          conn.commit()
          cur.close()
          conn.close()
          print(f"âœ… Successfully imported {imported_count} repository records")
          EOF

          python import_artifacts.py

      - name: Generate Timestamp
        id: timestamp
        run: |
          timestamp=$(date +"%Y%m%d_%H%M%S")
          echo "timestamp=$timestamp" >> $GITHUB_OUTPUT
          echo "Generated timestamp: $timestamp"

      - name: Export Database Tables to CSV
        env:
          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/github_crawler
        run: |
          timestamp="${{ steps.timestamp.outputs.timestamp }}"
          export_mode="${{ github.event.inputs.export_mode }}"

          echo "ğŸ”„ Exporting database tables to CSV files..."

          # Create export directory
          mkdir -p database_exports

          # Export repo table
          if [[ "$export_mode" == "all-tables" || "$export_mode" == "repo-only" ]]; then
            echo "Exporting repo table..."
            psql $DATABASE_URL -c "\COPY (SELECT * FROM repo ORDER BY id) TO STDOUT WITH CSV HEADER" > "database_exports/repo_${timestamp}.csv"
            echo "âœ… Exported repo table to repo_${timestamp}.csv"
          fi

          # Export repo_stats table
          if [[ "$export_mode" == "all-tables" || "$export_mode" == "stats-only" ]]; then
            echo "Exporting repo_stats table..."
            psql $DATABASE_URL -c "\COPY (SELECT * FROM repo_stats ORDER BY repo_id, fetched_date) TO STDOUT WITH CSV HEADER" > "database_exports/repo_stats_${timestamp}.csv"
            echo "âœ… Exported repo_stats table to repo_stats_${timestamp}.csv"
          fi

          # Export repo_archives table (if has data)
          if [[ "$export_mode" == "all-tables" ]]; then
            echo "Exporting repo_archives table..."
            psql $DATABASE_URL -c "\COPY (SELECT * FROM repo_archives ORDER BY repo_id, fetched_date) TO STDOUT WITH CSV HEADER" > "database_exports/repo_archives_${timestamp}.csv"
            echo "âœ… Exported repo_archives table to repo_archives_${timestamp}.csv"
            
            echo "Exporting repo_file_index table..."
            psql $DATABASE_URL -c "\COPY (SELECT * FROM repo_file_index ORDER BY repo_id, fetched_date, path) TO STDOUT WITH CSV HEADER" > "database_exports/repo_file_index_${timestamp}.csv"
            echo "âœ… Exported repo_file_index table to repo_file_index_${timestamp}.csv"
          fi

          # Generate summary statistics
          echo "Generating export summary..."
          cat > "database_exports/export_summary_${timestamp}.md" << EOF
          # Database Export Summary

          **Export Date:** $(date)
          **Export Mode:** $export_mode
          **Timestamp:** $timestamp

          ## Table Statistics
          EOF

          # Add table counts to summary
          psql $DATABASE_URL -c "SELECT 'Repositories: ' || COUNT(*) FROM repo;" -t >> "database_exports/export_summary_${timestamp}.md"
          psql $DATABASE_URL -c "SELECT 'Repository Stats: ' || COUNT(*) FROM repo_stats;" -t >> "database_exports/export_summary_${timestamp}.md"
          psql $DATABASE_URL -c "SELECT 'Repository Archives: ' || COUNT(*) FROM repo_archives;" -t >> "database_exports/export_summary_${timestamp}.md"
          psql $DATABASE_URL -c "SELECT 'File Index Entries: ' || COUNT(*) FROM repo_file_index;" -t >> "database_exports/export_summary_${timestamp}.md"

          echo "ğŸ“Š Export Summary:"
          cat "database_exports/export_summary_${timestamp}.md"

      - name: Configure Git
        run: |
          git config --local user.email "action@github.com"
          git config --local user.name "GitHub Action"

      - name: Commit Database Exports
        run: |
          timestamp="${{ steps.timestamp.outputs.timestamp }}"
          custom_message="${{ github.event.inputs.commit_message }}"

          # Add exported files
          git add database_exports/

          # Check if there are changes to commit
          if git diff --staged --quiet; then
            echo "No changes to commit"
            exit 0
          fi

          # Create commit message
          if [[ -n "$custom_message" ]]; then
            commit_msg="$custom_message"
          else
            export_mode="${{ github.event.inputs.export_mode }}"
            file_count=$(ls database_exports/*_${timestamp}.csv 2>/dev/null | wc -l)
            commit_msg="feat: database export ${timestamp} - ${export_mode} (${file_count} CSV files)"
          fi

          # Commit changes
          git commit -m "$commit_msg"

          echo "âœ… Committed database exports with message: $commit_msg"

      - name: Push Changes
        uses: ad-m/github-push-action@master
        with:
          github_token: ${{ secrets.GITHUB_TOKEN }}
          branch: ${{ github.ref }}

      - name: Upload Export Artifacts
        uses: actions/upload-artifact@v4
        with:
          name: database-exports-${{ steps.timestamp.outputs.timestamp }}
          path: database_exports/
          retention-days: 30

      - name: Display Export Results
        run: |
          echo "ğŸ‰ Database Export Complete!"
          echo ""
          echo "ğŸ“ Files committed to repository:"
          ls -la database_exports/
          echo ""
          echo "ğŸ”— Files are also available as workflow artifacts"
          echo "ğŸ“Š Check the export summary for detailed statistics"
