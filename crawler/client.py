import aiohttp
import asyncio
import logging
from typing import Optional, List, Dict, Any
from tenacity import (
    retry,
    stop_after_attempt,
    wait_exponential,
    retry_if_exception_type,
    before_sleep_log,
)

from .config import settings
from .domain import (
    Repository,
    CrawlResult,
    SearchQuery,
    transform_github_response,
    RateLimitError,
    AuthenticationError,
    SearchExhaustedError,
    ApiError,
)
from .search_strategy import SimpleSearchStrategy

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GitHubClient:
    """
    GitHub API client with comprehensive retry mechanisms and anti-corruption
    layer.

    This client implements clean architecture principles by:
    - Using domain models instead of raw API responses
    - Implementing proper retry mechanisms with tenacity
    - Providing connection pooling and resource management
    - Isolating external API concerns from business logic
    """

    def __init__(self, token: str = settings.github_token):
        if not token or token == "dummy_token_for_validation":
            raise ValueError("GitHub token is required and must be valid")

        self.graphql_url = "https://api.github.com/graphql"
        self.headers = {
            "Authorization": f"Bearer {token}",
            "Accept": "application/vnd.github.v4+json",
            "User-Agent": "GitHub-Crawler/1.0",
        }
        self.search_strategy = SimpleSearchStrategy()
        self._connector = None
        self._session = None
        logger.info(f"‚úÖ GitHub client initialized with token length: {len(token)}")

    async def __aenter__(self):
        """Async context manager entry."""
        self._connector = aiohttp.TCPConnector(
            limit=100,  # Total connection pool size
            limit_per_host=20,  # Connections per host
            keepalive_timeout=30,
            enable_cleanup_closed=True,
        )
        self._session = aiohttp.ClientSession(
            connector=self._connector,
            headers=self.headers,
            timeout=aiohttp.ClientTimeout(total=30),
        )
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit."""
        if self._session:
            await self._session.close()
        if self._connector:
            await self._connector.close()

    async def test_connection(self) -> bool:
        """Test GitHub API connection and authentication."""
        test_query = """
        query {
          viewer {
            login
          }
          rateLimit {
            remaining
            resetAt
          }
        }"""

        try:
            response = await self._make_graphql_request({"query": test_query})

            viewer_login = response["data"]["viewer"]["login"]
            rate_limit = response["data"]["rateLimit"]
            logger.info("‚úÖ GitHub API connection successful")
            logger.info(f"üìã Authenticated as: {viewer_login}")
            logger.info(f"üö¶ Rate limit remaining: {rate_limit['remaining']}")
            return True
        except Exception as e:
            logger.error(f"‚ùå GitHub API connection test failed: {e}")
            return False

    @retry(
        stop=stop_after_attempt(5),
        wait=wait_exponential(multiplier=1, min=1, max=60),
        retry=retry_if_exception_type((aiohttp.ClientError, RateLimitError)),
        before_sleep=before_sleep_log(logger, logging.WARNING),
        reraise=True,
    )
    async def _make_graphql_request(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        Make a GraphQL request with comprehensive retry logic.

        Uses tenacity for robust retry mechanisms with exponential backoff.
        """
        if not self._session:
            raise RuntimeError("Client must be used as async context manager")

        try:
            async with self._session.post(self.graphql_url, json=payload) as resp:
                # Handle authentication errors first
                if resp.status == 401:
                    raise AuthenticationError("GitHub API authentication failed")

                # Handle rate limiting
                if resp.status == 403:
                    response_text = await resp.text()
                    if "rate limit" in response_text.lower():
                        logger.warning("‚è±Ô∏è Rate limit hit, waiting...")
                        await asyncio.sleep(60)
                        raise RateLimitError("GitHub API rate limit exceeded")

                # Handle server errors
                if resp.status in {502, 503, 504}:
                    raise aiohttp.ClientResponseError(
                        resp.request_info,
                        resp.history,
                        status=resp.status,
                        message=f"Server error: {resp.status}",
                    )

                # For successful responses, handle rate limiting headers
                if resp.status == 200:
                    # Rate limit preemptive pause
                    remaining = int(resp.headers.get("X-RateLimit-Remaining", 1))
                    if remaining < 10:
                        await asyncio.sleep(0.5)

                    response_data = await resp.json()

                    # Handle GraphQL errors
                    if "errors" in response_data:
                        errors = response_data["errors"]
                        error_messages = [str(error) for error in errors]

                        # Check for specific error types
                        for error in errors:
                            error_str = str(error)
                            if "FORBIDDEN" in error_str or "Unauthorized" in error_str:
                                raise AuthenticationError(
                                    f"Authentication failed: {error}"
                                )
                            elif "RATE_LIMITED" in error_str:
                                logger.warning("‚è±Ô∏è GraphQL rate limited, waiting...")
                                await asyncio.sleep(60)
                                raise RateLimitError(f"GraphQL rate limited: {error}")

                        # If we have data despite errors, log and continue
                        if "data" in response_data and response_data["data"]:
                            logger.warning(
                                f"‚ö†Ô∏è GraphQL errors (continuing): " f"{error_messages}"
                            )
                        else:
                            raise ApiError(f"GraphQL query failed: {error_messages}")

                    return response_data

                # Handle any other non-200 status codes
                resp.raise_for_status()
                return {}  # Should never reach here due to raise_for_status
        except aiohttp.ClientError as e:
            logger.warning(f"üîÅ Network error: {e}")
            raise

    async def search_repositories(
        self, query: SearchQuery, after: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Execute a GraphQL search query and return repositories using domain models.

        This method implements the anti-corruption layer pattern by:
        - Taking domain SearchQuery objects instead of raw strings
        - Returning structured data with proper typing
        - Handling errors with custom exception types
        """
        graphql_query = """
        query ($searchQuery: String!, $after: String) {
          search(query: $searchQuery, type: REPOSITORY, first: 100, after: $after) {
            pageInfo {
              endCursor
              hasNextPage
            }
            repositoryCount
            nodes {
              ... on Repository {
                databaseId
                name
                url
                createdAt
                stargazerCount
                forkCount
                primaryLanguage {
                  name
                }
                owner {
                  login
                }
                licenseInfo {
                  name
                }
                pushedAt
                updatedAt
              }
            }
          }
          rateLimit {
            remaining
            resetAt
          }
        }"""

        variables = {"searchQuery": query.query_string, "after": after}
        payload = {"query": graphql_query, "variables": variables}

        try:
            response = await self._make_graphql_request(payload)

            if "data" not in response:
                raise ApiError(f"No data in GraphQL response: {response}")

            search_data = response["data"]["search"]
            rate_limit = response["data"]["rateLimit"]

            # Convert raw API response to domain objects
            repositories = []
            for node in search_data["nodes"]:
                repo = transform_github_response(node)
                repositories.append(repo)

            logger.info(f"üîç Query returned {len(repositories)} repositories")
            logger.info(f"üö¶ Rate limit remaining: {rate_limit['remaining']}")

            return {
                "repositories": repositories,
                "pageInfo": search_data["pageInfo"],
                "repositoryCount": search_data["repositoryCount"],
                "rateLimit": rate_limit,
            }

        except (RateLimitError, AuthenticationError, SearchExhaustedError):
            # Re-raise domain exceptions
            raise
        except Exception as e:
            logger.error(
                f"‚ùå GraphQL query failed for query '{query.query_string}': {e}"
            )
            raise ApiError(f"Search request failed: {e}") from e

    async def crawl(self, matrix_total: int = 1, matrix_index: int = 0) -> CrawlResult:
        """
        Main crawling method using clean architecture principles.

        This method:
        - Uses domain models instead of raw dictionaries
        - Delegates search strategy to dedicated class
        - Implements proper resource management
        - Returns structured results with metadata
        """
        logger.info(f"üöÄ Starting crawl: Matrix job {matrix_index + 1}/{matrix_total}")
        logger.info(f"üéØ Target: {settings.max_repos} repositories")

        repositories: List[Repository] = []
        repository_ids: set[int] = set()
        target_repos = settings.max_repos

        # Use search strategy to generate optimized queries
        search_queries = self.search_strategy.generate_queries(
            matrix_index, matrix_total
        )

        for query_idx, search_query in enumerate(search_queries):
            if len(repositories) >= target_repos:
                break

            logger.info(
                f"üîç Query {query_idx + 1}/{len(search_queries)}: "
                f"{search_query.query_string}"
            )

            try:
                await self._crawl_query(
                    search_query, repositories, repository_ids, target_repos
                )
            except SearchExhaustedError:
                logger.warning(
                    f"‚ö†Ô∏è Search exhausted for query: {search_query.query_string}"
                )
                continue
            except Exception as e:
                logger.error(
                    f"‚ùå Error processing query {search_query.query_string}: {e}"
                )
                continue

        # Truncate to target and calculate stats
        final_repositories = repositories[:target_repos]

        # Create crawl result with all repositories and metadata
        crawl_result = CrawlResult(
            repositories=final_repositories,
            total_found=len(repositories),  # Total before truncation
            duration_seconds=0.0,  # This could be calculated if needed
            errors=[],
        )

        if final_repositories:
            logger.info(f"üéâ Crawl completed for matrix job {matrix_index}")
            logger.info(f"üìä Collected: {len(final_repositories)} unique repositories")
            logger.info(f"üë• Unique owners: {crawl_result.unique_owners}")
            logger.info(f"‚≠ê Total stars: {crawl_result.total_stars:,}")
            if crawl_result.total_stars > 0:
                average_stars = crawl_result.total_stars / len(final_repositories)
                logger.info(f"üìà Average stars: {average_stars:.1f}")
        else:
            logger.warning("‚ö†Ô∏è No repositories collected")

        if len(final_repositories) < target_repos:
            logger.warning(
                f"‚ö†Ô∏è Only collected {len(final_repositories)}/{target_repos} repos. "
                f"Search space may be exhausted for this partition."
            )

        return crawl_result

    async def _crawl_query(
        self,
        search_query: SearchQuery,
        repositories: List[Repository],
        repository_ids: set,
        target_repos: int,
    ):
        """Process a single search query with pagination."""
        after_cursor = None
        pages_processed = 0
        max_pages = 10  # Prevent infinite loops

        while len(repositories) < target_repos and pages_processed < max_pages:
            try:
                result = await self.search_repositories(search_query, after_cursor)

                # Process repositories from this page
                batch_added = 0
                for repo in result["repositories"]:
                    if repo.id not in repository_ids:
                        repositories.append(repo)
                        repository_ids.add(repo.id)
                        batch_added += 1

                        if len(repositories) >= target_repos:
                            break

                logger.debug(
                    f"üìÑ Page {pages_processed + 1}: "
                    f"Added {batch_added} new repositories"
                )

                # Check if we should continue paginating
                page_info = result["pageInfo"]
                if not page_info["hasNextPage"]:
                    break

                after_cursor = page_info["endCursor"]
                pages_processed += 1

                # Respect rate limits
                if result["rateLimit"]["remaining"] < 100:
                    logger.info("‚è±Ô∏è Rate limit low, sleeping...")
                    await asyncio.sleep(1)

            except RateLimitError:
                logger.warning("‚è±Ô∏è Rate limit hit, sleeping 60 seconds...")
                await asyncio.sleep(60)
                continue
            except Exception as e:
                logger.error(f"‚ùå Error in query pagination: {e}")
                break
