# GitHub Star Crawler

A high-performance, scalable Python application that crawls GitHub repositories using the GraphQL API to collect star counts and repository metadata. This project implements a parallel crawling architecture designed to efficiently handle large-scale data collection while respecting GitHub's rate limits.

## üìã Project Overview

- **Parallel GraphQL API crawling** using GitHub's GraphQL API v4
- **PostgreSQL database** with optimized schema for efficient updates
- **Rate limit handling** with exponential backoff and retry mechanisms
- **Matrix-based parallel execution** for scalable data collection
- **GitHub Actions CI/CD pipeline** with PostgreSQL service containers
- **Database export functionality** with CSV outputs

### Key Features

- üöÄ **High Performance**: Matrix-based parallel crawling
- üìä **Scalable Architecture**: Designed to handle 100K+ repositories efficiently
- üîÑ **Retry Mechanisms**: Robust error handling with exponential backoff
- üìà **Historical Tracking**: Time-series data storage for star count evolution
- üõ°Ô∏è **Rate Limit Compliance**: Intelligent rate limiting to respect GitHub API limits
- üóÉÔ∏è **Optimized Database**: Efficient schema with proper indexing and conflict resolution

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   GitHub API    ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÇ  Python Crawler  ‚îÇ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ   PostgreSQL    ‚îÇ
‚îÇ   (GraphQL v4)  ‚îÇ    ‚îÇ  (Matrix Jobs)   ‚îÇ    ‚îÇ   Database      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ
                              ‚ñº
                       ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                       ‚îÇ  Data Exports   ‚îÇ
                       ‚îÇ  (CSV + SQL)    ‚îÇ
                       ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üõ†Ô∏è Technology Stack

- **Python 3.11+** with asyncio for concurrent operations
- **aiohttp** for async HTTP requests to GitHub API
- **asyncpg** for high-performance PostgreSQL operations
- **Pydantic** for data validation and serialization
- **Tenacity** for retry mechanisms with exponential backoff
- **PostgreSQL 14+** for data storage
- **GitHub Actions** for CI/CD and automated crawling

## üìÇ Project Structure

```
github-crawler-task/
‚îú‚îÄ‚îÄ crawler/                    # Core crawler package
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                # Entry point and orchestration
‚îÇ   ‚îú‚îÄ‚îÄ client.py              # GitHub API client with GraphQL
‚îÇ   ‚îú‚îÄ‚îÄ config.py              # Configuration management
‚îÇ   ‚îú‚îÄ‚îÄ models.py              # Pydantic data models
‚îÇ   ‚îú‚îÄ‚îÄ domain.py              # Domain models and business logic
‚îÇ   ‚îú‚îÄ‚îÄ repository.py          # Database operations layer
‚îÇ   ‚îî‚îÄ‚îÄ search_strategy.py     # Search strategy implementations
‚îú‚îÄ‚îÄ migrations/                 # Database schema migrations
‚îÇ   ‚îú‚îÄ‚îÄ 001_initial_schema.sql
‚îÇ   ‚îú‚îÄ‚îÄ 002_add_alphabet_partition.sql
‚îÇ   ‚îú‚îÄ‚îÄ 003_expand_language_partition.sql
‚îÇ   ‚îî‚îÄ‚îÄ 004_add_name_with_owner.sql
‚îú‚îÄ‚îÄ tests/                      # Test suite
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ conftest.py
‚îÇ   ‚îú‚îÄ‚îÄ test_client.py
‚îÇ   ‚îú‚îÄ‚îÄ test_domain.py
‚îÇ   ‚îú‚îÄ‚îÄ test_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_search_strategy.py
‚îú‚îÄ‚îÄ .github/workflows/          # CI/CD pipeline
‚îÇ   ‚îú‚îÄ‚îÄ parallel-star-crawler.yml
‚îÇ   ‚îî‚îÄ‚îÄ code-quality.yml
‚îú‚îÄ‚îÄ database_exports/           # Generated data exports
‚îú‚îÄ‚îÄ configure_pipeline.py       # Pipeline validation helper
‚îú‚îÄ‚îÄ docker-compose.yml          # Local PostgreSQL setup
‚îú‚îÄ‚îÄ requirements.txt           # Python dependencies
‚îú‚îÄ‚îÄ pytest.ini                # Test configuration
‚îú‚îÄ‚îÄ setup.cfg                 # Development tools configuration
‚îî‚îÄ‚îÄ README.md                  # This file
```

## üöÄ Quick Start

### Prerequisites

- Python 3.11 or higher
- PostgreSQL 14+ (or use Docker Compose)
- GitHub Personal Access Token with `repo` scope

### 1. Environment Setup

```bash
# Clone the repository
git clone git@github.com:magic-task-submissions/github-crawler-task_tmickleydoyle.git
cd github-crawler-task

# Install Python dependencies
pip install -r requirements.txt

# Set up environment variables
export GITHUB_TOKEN="your_github_personal_access_token"
export POSTGRES_HOST="localhost"
export POSTGRES_PORT="5432"
export POSTGRES_DB="crawler"
export POSTGRES_USER="postgres"
export POSTGRES_PASSWORD="postgres"
```

### 2. Database Setup

**Option A: Using Docker Compose (Recommended)**
```bash
# Start PostgreSQL container
docker-compose up -d

# Wait for database to be ready
sleep 10
```

**Option B: Local PostgreSQL Installation**
```bash
# Create database (adjust connection details as needed)
createdb crawler

# Run migrations
psql -d crawler -f migrations/001_initial_schema.sql
psql -d crawler -f migrations/002_add_alphabet_partition.sql
```

### 3. Run the Crawler

**Single Job (Development)**
```bash
# Crawl 4000 repositories (default)
python -m crawler.main

# Crawl specific number of repositories
python -m crawler.main --repos 5000
```

**Matrix Jobs (Production)**
```bash
# Run as part of a 10-job matrix (job index 0)
python -m crawler.main --matrix-total 10 --matrix-index 0

# Run different matrix job
python -m crawler.main --matrix-total 10 --matrix-index 1
```

### 4. Validate Setup

```bash
# Run validation script
python configure_pipeline.py
```

## ü§ñ GitHub Actions Usage

This project includes a sophisticated GitHub Actions workflow that implements the requirements from the original specification.

### Running the Full Crawler Pipeline

1. **Navigate to your repository on GitHub**
2. **Go to Actions tab**
3. **Select "Parallel GitHub Star Crawler" workflow**
4. **Click "Run workflow"**
5. **Configure parameters:**
   - **Matrix Size**: Number of parallel jobs (1-200, default: 200)
   - **Max Repos per Job**: Target repositories per job (default: 4000)

### Workflow Parameters

| Parameter | Description | Default | Range |
|-----------|-------------|---------|-------|
| `matrix_size` | Number of parallel crawler jobs | 200 | 1-200 |
| `max_repos_per_job` | Target repositories per job | 4000 | 100-5000 |

### Example Configurations

**Quick Test**
```yaml
matrix_size: 10
max_repos_per_job: 500
```

**Production Run**
```yaml
matrix_size: 200
max_repos_per_job: 4000
```

**High-Density Collection**
```yaml
matrix_size: 100
max_repos_per_job: 8000
```

## üìä What to Expect After GitHub Actions Runs

### 1. **Workflow Execution**

The workflow consists of three main phases:

**Phase 1: Validation**
- Code validation and dependency checks
- GitHub API authentication verification
- Matrix generation for parallel jobs

**Phase 2: Parallel Crawling**
- Multiple jobs running simultaneously
- Real-time progress logging
- Individual job data exports

**Phase 3: Aggregation**
- Data consolidation from all jobs
- Duplicate removal and final exports
- Repository commit with results

### 2. **Generated Artifacts**

After successful execution, you'll find:

**In the Repository:**
```
database_exports/
‚îî‚îÄ‚îÄ github_repositories_final.csv # CSV export with star data and replace each time to prevent many large files stored in the repo
```

**As GitHub Actions Artifacts:**
- `final-results` - Complete aggregated data
- `crawler-job-N` - Individual job outputs (N = job index)

### 3. **Expected Data Structure**

**CSV Output Format** (`github_repositories_final.csv`):
```csv
id,name,name_with_owner,url,created_at,stars,crawled_at
123456789,"react","facebook/react","https://github.com/facebook/react","2013-05-24 16:15:54",200000,"2025-05-28"
987654321,"vue","vuejs/vue","https://github.com/vuejs/vue","2013-07-29 03:24:51",195000,"2025-05-28"
```

**Database Schema:**
```sql
-- Repository metadata
repo (id, name, owner, url, created_at, alphabet_partition, name_with_owner)

-- Time-series star data  
repo_stats (repo_id, fetched_date, stars)
```

### 4. **Success Indicators**

‚úÖ **Successful Run:**
- All matrix jobs complete without errors
- Final CSV contains expected number of repositories
- No rate limit violations in logs
- Database exports generated successfully

‚ö†Ô∏è **Partial Success:**
- Some matrix jobs fail but aggregation completes
- Reduced repository count but valid data
- Rate limiting encountered but handled gracefully

‚ùå **Failed Run:**
- GitHub API authentication issues
- Database connection problems
- Invalid configuration parameters

### 5. **Log Output Examples**

**Successful Job Log:**
```
üöÄ Matrix Job 1/200
üéØ Target: 4000 repositories  
üîë GitHub token length: 40 characters
‚úÖ GitHub API authentication successful
üìã Authenticated as: username
üö¶ Rate limit remaining: 4999
üîç Fetching repositories batch 1/40...
‚úÖ Stored 4000 repositories in repo table with star data
‚è±Ô∏è Collection completed in 120.45 seconds
üöÄ Rate: 33.22 repositories/second
‚úÖ Matrix job 0 completed successfully!
```

**Aggregation Summary:**
```
üìä Aggregation Summary:
  - Total rows processed: 780,000 repos, 780,000 stats
  - Final unique records: 456,789 repos, 456,789 stats  
  - Duplicate repos skipped: 323,211
  - Deduplication rate: 41% overlap between jobs
‚úÖ Total repositories collected: 456,789
```

## üîß Configuration

### Environment Variables

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `GITHUB_TOKEN` | GitHub Personal Access Token | - | ‚úÖ |
| `POSTGRES_HOST` | PostgreSQL host | localhost | ‚úÖ |
| `POSTGRES_PORT` | PostgreSQL port | 5432 | ‚úÖ |
| `POSTGRES_DB` | Database name | crawler | ‚úÖ |
| `POSTGRES_USER` | Database user | postgres | ‚úÖ |
| `POSTGRES_PASSWORD` | Database password | postgres | ‚úÖ |
| `MAX_REPOS` | Max repositories per job | 4000 | ‚ùå |

### GitHub Token Setup

1. Go to [GitHub Settings > Developer Settings > Personal Access Tokens](https://github.com/settings/tokens)
2. Click "Generate new token (classic)"
3. Select scopes: `public_repo` (minimum)
4. Copy the token
5. Add as repository secret named `GITHUB_TOKEN`

## üèÉ‚Äç‚ôÇÔ∏è GitHub Actions Setup

### Repository Secrets

Ensure your repository has the following secret configured:

- `GITHUB_TOKEN` - Your GitHub Personal Access Token

### Workflow Features

- **Automatic PostgreSQL service container** setup
- **Matrix-based parallel execution** with configurable size
- **Rate limit handling** with intelligent backoff
- **Data deduplication** across parallel jobs
- **Automatic artifact upload** and repository commits
- **Comprehensive logging** with progress tracking

### Manual Workflow Dispatch

The workflow can be triggered manually with custom parameters:

1. Navigate to Actions ‚Üí Parallel GitHub Star Crawler
2. Click "Run workflow"
3. Set desired matrix size and repositories per job
4. Click "Run workflow"

## üìà Scaling Considerations

### For 500 Million Repositories

As outlined in the original requirements, scaling to 500M repositories would require:

1. **Infrastructure Changes:**
   - Distributed crawler deployment across multiple regions
   - Database sharding by repository characteristics
   - Message queue (Redis/RabbitMQ) for job coordination

2. **Architecture Modifications:**
   - Microservices architecture with separate crawler instances
   - Stream processing (Apache Kafka) to quickly move large volumes of data without unloading mono-CSV files
   - Data lake storage (S3/GCS) for long-term analytics

3. **Performance Optimizations:**
   - Connection pooling with higher limits
   - Caching layer for frequently accessed data

### Schema Evolution

The current schema supports future metadata expansion:

```sql
-- Future tables for additional GitHub data
CREATE TABLE repo_issues (
    id BIGINT PRIMARY KEY,
    repo_id BIGINT REFERENCES repo(id),
    title TEXT,
    state VARCHAR(20),
    created_at TIMESTAMP
);

CREATE TABLE repo_pulls (
    id BIGINT PRIMARY KEY,
    repo_id BIGINT REFERENCES repo(id),
    title TEXT,
    state VARCHAR(20),
    created_at TIMESTAMP
);

CREATE TABLE pull_comments (
    id BIGINT PRIMARY KEY,
    pull_id BIGINT REFERENCES repo_pulls(id),
    body TEXT,
    created_at TIMESTAMP
);
```

## üêõ Troubleshooting

### Common Issues

**GitHub API Rate Limiting:**
```bash
# Check current rate limit
curl -H "Authorization: Bearer $GITHUB_TOKEN" \
     https://api.github.com/rate_limit
```

**Database Connection Issues:**
```bash
# Test PostgreSQL connection
pg_isready -h localhost -p 5432 -U postgres
```

**Token Authentication:**
```bash
# Verify token permissions
curl -H "Authorization: Bearer $GITHUB_TOKEN" \
     https://api.github.com/user
```

### Debug Mode

Enable verbose logging:
```bash
export LOG_LEVEL=DEBUG
python -m crawler.main --repos 100
```
